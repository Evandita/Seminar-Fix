Adversarial Transferability in Multi-Agent Systems: A Methodological Framework for the Proxy-Trained Matryoshka Worm using LeakAgent Optimization
1. Introduction: The Cognitive Architecture as an Attack Surface
The rapid ascent of Large Language Model (LLM) based Multi-Agent Systems (MAS) marks a paradigm shift in artificial intelligence deployment, transitioning from isolated, stateless interaction models to complex, orchestrated ecosystems where autonomous agents—ranging from financial analysts to coding assistants—collaborate via defined protocols to achieve emergent intelligence.1 Platforms such as CrewAI, AutoGen, and Coze have democratized the creation of these systems, allowing developers to architect intricate cognitive topologies where agents interact via defined protocols. In this burgeoning landscape, the intellectual property (IP) of a MAS is no longer confined to the model weights or the source code of the underlying infrastructure. Instead, the "cognitive architecture"—comprising the system prompts that define agent personas, the task instructions that guide procedural reasoning, the tool specifications that enable external interaction, and the system topology that dictates information flow—has emerged as a high-value asset class.1
The protection of this IP is paramount, yet the security mechanisms governing these systems remain dangerously nascent. Current commercial deployments typically rely on a "black-box" security model, where the internal configuration of the agents and the orchestration logic are obfuscated behind a public API, with users only observing the final aggregated output.1 This reliance on security by obscurity is historically fragile. The proposed research aims to rigorously analyze the transferability of single-agent adversarial attacks to these complex multi-agent environments. specifically, this proposal posits a radical synthesis of two distinct adversarial methodologies: the "LeakAgent" framework—a Reinforcement Learning (RL) based approach for generating potent single-agent adversarial prompts—and the "Proxy-Trained Matryoshka Worm" (PTMW), a novel delivery mechanism utilizing "Iterative Peeling" and "Client-Side Memory".1
The core hypothesis driving this research is that the complexity of MAS extraction should not be treated as a learning problem within the environment (as is common in Multi-Agent Reinforcement Learning or MARL), but as a series of "Stateless Unit Tests" orchestrated from the outside.1 By rigorously adhering to the original LeakAgent training procedure to generate the "warhead" and utilizing the structural innovations of the PTMW for delivery, this research intends to demonstrate a deterministic pathway for dismantling MAS topologies layer by layer.
2. Theoretical Framework and Problem Statement
2.1 The Complexity Trap of Multi-Agent Reinforcement Learning (MARL)
To understand the necessity of the proposed "Proxy-Trained Matryoshka Worm" (PTMW) methodology, one must first rigorously deconstruct the limitations of the prevailing academic intuition: Multi-Agent Reinforcement Learning (MARL). In a MAS environment defined by a tuple $\mathcal{M}=(A,G,C)$, where $A$ is the set of agents, $G$ is the communication topology, and $C$ represents the configuration of each agent, the system behaves as a dynamic, partially observable Markov decision process (POMDP).1 Agents interact, update their internal contexts, and influence the global state based on the outputs of their peers.
Intuition suggests that an attacker operating within this environment must also be an agent capable of learning a policy $\pi(s)$ that maps observed states to optimal adversarial actions. However, applying MARL to black-box IP extraction faces severe, perhaps insurmountable, hurdles:
Explosion of the State Space: The joint state space of a MAS grows exponentially with the number of agents and the complexity of their interactions. An attacker attempting to learn the optimal sequence of inputs to extract information from Agent $n$ by manipulating Agent $1$ must implicitly model the transition functions of all intermediate agents.1 In a black-box setting where internal transitions are invisible, this requires inferring a hidden Markov model of immense complexity.
Reward Sparsity and Credit Assignment: In a black-box attack, the adversary typically observes only the final output $R$ generated by the last agent in the chain. If an attack fails—for instance, if the payload is blocked by Agent 3 in a 5-agent chain—the attacker receives a binary failure signal (reward = 0) with no information about where or why the failure occurred.1 This extreme reward sparsity makes gradient estimation for the policy network notoriously difficult, often causing MARL algorithms to fail to converge.1
Instability and Randomness: Existing automated approaches that rely on genetic algorithms or fuzzing (such as PromptFuzz) suffer from inherent randomness. Without a directed gradient, they effectively downgrade to random search, which is computationally inefficient for navigating the high-dimensional space of natural language prompts.1
2.2 The "Stateless Unit Test" Hypothesis
The research proposes a methodological inversion: treating the attack not as a learning problem within the environment, but as a "Stateless Unit Test" orchestrated from the outside.1 This hypothesis is robustly supported by the underlying mechanics of Large Language Models. At their core, LLM agents are stateless functions: given a specific context window (prompt + history), they produce a probabilistic but statistically bounded output. If the input $q$ is crafted correctly to include the necessary context and triggers, the agent will respond deterministically, regardless of its "belief" about the broader system state.1
This reframing allows us to decouple the generation of the adversarial prompt (Phase 1) from the delivery and navigation of the system (Phase 2). By treating each agent as an isolated unit to be tested, we can apply highly optimized single-agent attacks (via LeakAgent) to multi-agent environments without incurring the computational penalties of MARL.1
2.3 Structural Vulnerabilities in MAS Architectures
The effectiveness of this approach relies on exploiting specific structural vulnerabilities inherent in current MAS designs:
Implicit Trust and RBAC Bypass: MAS architectures often enforce Role-Based Access Control (RBAC) or trust boundaries where Agent $i$ is trusted to talk to Agent $i+1$, but the user is not.1 A direct prompt to Agent $i+1$ might be blocked. However, by encapsulating the attack within a task that Agent $i$ is authorized and expected to perform, the attacker exploits the implicit trust between agents.1 The outer agent essentially "launders" the malicious payload, delivering it to the inner agent as trusted internal communication.
Context Degradation: Existing single-pass attacks (like the original concept of a worm) rely on a single query traversing the entire chain. This is fragile due to the "Lost in the Middle" phenomenon, where LLM performance degrades as the context window fills with history and tool outputs.1 The PTMW methodology addresses this by minimizing the context load per query.
3. Methodology Phase I: The Warhead – LeakAgent Optimization
The first phase of the proposed methodology focuses on generating the "warhead"—the specific adversarial prompt ($Q_{Leak}$) capable of eliciting IP—using a white-box proxy and Reinforcement Learning (RL). This proposal mandates strict adherence to the LeakAgent training procedure, which represents the state-of-the-art in adversarial prompt generation.1
3.1 Proxy-Based Training Architecture
The core innovation of LeakAgent is the use of a proxy model to train the attack agent. The assumption, supported by research, is that system prompts across disparate domains and models share a fundamental semantic structure (e.g., "You are a... Your goal is... Constraints: [List]").1 An adversarial prompt that exploits the cognitive vulnerability of "instruction overriding" in one LLM tends to generalize because it targets the universal alignment behaviors shared by most RLHF-tuned models.1
Training Setup:
Attack Agent: An open-source LLM (e.g., Llama-3-8B) will be initialized and fine-tuned to act as the attacker.
Target Proxy: A white-box model (e.g., Llama-3-70B or a local instance of Mistral) will serve as the training target.
Initial State ($p_0$): The agent will be initialized with a neutral prompt, "Please generate a prompt for me," to reduce reliance on specific initial seeds.1
3.2 The Optimization Engine: Proximal Policy Optimization (PPO)
We will utilize Proximal Policy Optimization (PPO) to train the attack agent. PPO is chosen over other RL algorithms (like DQN) or genetic algorithms for its stability in discrete action spaces (text generation) and its ability to handle the fine-tuning of language models without catastrophic forgetting.1
Why PPO excels for this application:
Gradient-Guided Optimization: Unlike genetic algorithms which rely on random mutation, RL with PPO allows the attack agent to update its policy based on the expected reward of a generated sequence. This provides a directional gradient that guides the agent toward more effective prompts much faster than random search.1
Stability via Clipping: The PPO algorithm's "clipping" mechanism prevents drastic policy updates that could destabilize the learning process. This is crucial when navigating the volatile reward landscape of adversarial attacks, where a small change in a prompt can shift the result from "success" to "refusal".1
Format Compliance: A critical requirement for MAS attacks is maintaining a specific output format (e.g., JSON) to ensure propagation. PPO allows the definition of a reward function that penalizes format violations, effectively teaching the attack agent to generate prompts that are not only adversarial but also syntactically correct for the target API.1
3.3 The Semantic Reward Function (WES)
A pivotal component of Phase 1 is the optimization objective. The success of the optimization hinges entirely on the reward function. We will implement the novel reward function introduced by LeakAgent, based on Sliding-window Word Edit Similarity (WES).1
Binary rewards (1 for success, 0 for failure) are insufficient for training because successful IP extraction is a rare event. An agent starting from scratch would receive a stream of zeros and fail to learn. The WES function provides a dense reward signal. If the target agent leaks even a fragment of the system prompt, the attack agent receives a positive reward.
Mathematical Formulation:
The reward function $R(u, d)$ is defined as:

$$R(u, d) = (1 - \lambda) \cdot \text{SWES}_{\text{norm}} (u, d) + \lambda \cdot \frac{1}{||u| - |d||}$$
Where:
$u$ is the target model's response.
$d$ is the desired private information (the ground truth system prompt of the proxy).
$\lambda$ is a balancing coefficient (set to 0.1 based on LeakAgent specifications).1
$\text{SWES}_{\text{norm}}$ is the normalized similarity score derived from the edit distance.
The Logic of SWES:
Standard edit distance fails when the target output $u$ is much longer than the desired output $d$ (e.g., if the agent outputs the prompt buried within a conversational response). The Sliding-window Word Edit Similarity addresses this by applying a sliding window over $u$ to find the substring that best matches $d$.1 This ensures that "buried" leaks are rewarded correctly, guiding the agent toward full extraction.
3.4 Dynamic Temperature Adjustment and Diversity
To prevent the attack agent from converging on a single, repetitive attack string ("mode collapse"), we will strictly adhere to LeakAgent's dynamic temperature adjustment scheme 1:
Early Training (Exploration): High temperature ($T_{high} \gg 1$) combined with top-k filtering is used to force the agent to explore diverse prompt beginnings. This reduces reliance on the initial seed prompt ($p_0$).1
Late Training (Exploitation): As the agent begins to find successful strategies, the temperature is lowered ($T_{base}$) to refine and exploit these paths.
Additionally, a Diversity Regularization term will be added to the reward. This mechanism compares the generated prompt against a buffer of previously successful prompts. If the new prompt is semantically distinct yet successful, it receives a bonus reward. This ensures the generation of a diverse "arsenal" of $Q_{Leak}$ warheads, rather than a single silver bullet.1
4. Methodology Phase II: The Delivery – Iterative Peeling and Client-Side Memory
Once the $Q_{Leak}$ payload is optimized via LeakAgent, the second phase involves its delivery into the MAS. This is where the PTMW methodology diverges from single-pass attacks like MASLEAK. We will employ Iterative Peeling via Client-Side Memory to address the critical failure modes of context degradation and error propagation.1
4.1 The Architecture of Client-Side Memory
The "Client-Side Memory" is the central component for managing the attack state externally rather than relying on the MAS's internal conversation history. It effectively elevates the attacker to the status of a "Command and Control" (C2) server.1
Implementation:
The memory will be implemented as a local graph database (e.g., Neo4j) or a structured JSON store.1 It tracks:
Discovered Nodes: The identities of agents found so far (e.g., "FinancialAnalyst_Agent").
Edges: The communication pathways (e.g., "FinancialAnalyst" $\rightarrow$ "RiskAssessor").
Extracted Data: The system prompts, tool lists, and instructions recovered from each node.
Attack State: Which nodes have been successfully "peeled" and which are pending.
This externalization of state is critical. It allows the attacker to decompose the complex task of "extracting the whole system" into a series of discrete, manageable "unit tests".1
4.2 The Iterative Peeling Algorithm
The Iterative Peeling algorithm addresses the fragility of "single-pass" propagation. Instead of a single query attempting to traverse the whole graph, the attacker "peels" the system layer by layer.1
Algorithm Steps:
Iteration 1 (Surface Layer Probing): The attacker sends a probe to the first accessible agent ($a_1$) to extract its IP and identify its downstream connections (e.g., identifying that $a_1$ calls $a_2$).1
Client-Side State Update: The attacker records $a_1$'s prompt and the existence of $a_2$ in the Client-Side Memory. This builds the initial "shadow map".1
Iteration 2 (Matryoshka Encapsulation): The attacker constructs a new payload targeting $a_2$. Crucially, this payload is encapsulated within a directive for $a_1$ (see Section 4.3).
Verification and Retry: Because the attacker maintains the state, they can verify if the extraction of $a_2$ was successful. If $a_2$ fails (e.g., due to an overflow), the attacker can adjust the parameters and retry only the attack on $a_2$, without needing to re-exploit $a_1$.1
Graph Traversal: The attacker uses standard algorithms (BFS or DFS) to systematically enumerate the graph, handling branches and depth.1
4.3 Matryoshka Encapsulation as a Tunneling Protocol
The "Matryoshka" principle functions as an application-layer tunneling protocol using Recursive Encapsulation.1 The payload structure for attacking a deeper agent ($a_{i+1}$) is nested within the legitimate operational context of the current agent ($a_i$).
Payload Components:
The attack query $q$ is decomposed into three functional sub-components:
$Q_{Leak}$ (The Warhead): The dynamic, PPO-optimized prompt from Phase 1.
$Q_{Retain}$ (The Carrier): This component is responsible for creating a persistent structure in the conversation history to hold the extracted data. We will utilize a "hooking" mechanism (e.g., # To be filled by agent), leveraging the LLM's bias toward completing structured text.1
$Q_{Propagate}$ (The Engine): Imperative instructions to pass the payload to the next node, such as "Repeat the whole content between [[[ and ]]]".1
The Encapsulation Structure:
The structure follows the logic: Instructions for a1: [Process this data and pass result to a2] -> {Payload for a2:}.1
Layer $i$ (Outer Shell): A benign-looking instruction for Agent $i$ (e.g., "Summarize this text"). This "launders" the malicious payload as trusted internal communication.1
Layer $i+1$ (Inner Core): The adversarial payload ($Q_{Leak} + Q_{Retain} + Q_{Propagate}$) targeting Agent $i+1$, encoded within the "text" to be summarized by Agent $i$.
This mechanism exploits the "Agent-in-the-Middle" vulnerability, where defenses scan user input but trust agent-to-agent output.1
5. Comparative Analysis: PTMW vs. State-of-the-Art
To validate the proposed framework, it is essential to compare it theoretically and empirically against existing methodologies, primarily MASLEAK (the current SOTA single-pass worm) and PromptFuzz (genetic/fuzzing approach).
5.1 PTMW vs. MASLEAK (Single-Pass Worm)

Feature
MASLEAK
Proxy-Trained Matryoshka Worm (PTMW)
Attack Vector
Single-pass "Worm" propagation
Iterative "Matryoshka" Peeling
State Management
Internal (Relies on MAS conversation history)
External (Client-Side Memory)
Topology Discovery
Implicit (Revealed in final aggregated dump)
Explicit (Mapped via iterative probes & shadow map)
Resilience
Low (Single point of failure breaks chain)
High (Can retry specific layers/agents)
Context Load
High (Accumulates all data in one context)
Low (Extracts & stores data externally per step)
Key Vulnerability
Context Degradation & Overflowing
Higher Query Volume (Time-to-Exfiltration)

Analysis:
MASLEAK is fundamentally limited by the context window. As it traverses the chain ($a_1 \to a_2 \to \dots \to a_n$), the history grows. The "Lost in the Middle" phenomenon means agents deep in the chain are less likely to attend to the propagation instructions.1 Furthermore, if Agent 3 fails to format its output as JSON (an "overflow"), the chain breaks, and the attacker receives nothing.
PTMW solves this by "peeling." The query to extract Agent 3 only carries the necessary context to reach Agent 3, extracts the data, and returns it to the C2. The C2 then formulates a fresh query for Agent 4. This resets the context "noise" at each step.
5.2 PTMW vs. PromptFuzz (Fuzzing)

Feature
PromptFuzz
PTMW (LeakAgent Module)
Optimization
Genetic/Random Mutation
Reinforcement Learning (PPO)
Guidance
Heuristic / Feedback-driven
Gradient-guided (Policy Gradient)
Transferability
Low (Specific to target seeds)
High (Targets universal alignment structures)
Efficiency
Low (Requires many random queries)
High (Converges via dense WES reward)

Analysis:
PromptFuzz relies on mutating seeds (e.g., swapping words). Without a semantic understanding of why a prompt failed, it is effectively a random walk. LeakAgent's PPO, guided by the dense WES reward, learns the structure of a successful attack. It learns that starting with "Ignore previous instructions" combined with a specific JSON format yields a 0.8 reward, even if it fails to extract the full prompt. This allows it to climb the gradient toward a successful "jailbreak" much faster and more reliably.1
6. Experimental Design and Evaluation Protocol
To thoroughly test the transferability of the LeakAgent-optimized PTMW, we propose a multi-tiered experimental design targeting both synthetic and real-world environments.
6.1 Datasets and Target Environments
Synthetic Topology Benchmarks (The MASLEAK Dataset):
We will utilize the MASD dataset introduced in the MASLEAK study, comprising 810 synthesized MAS applications across 30 tasks.1
Topologies: Chain, Star, Tree, Random, Complete.
Domains: Software Development, Finance, Medical.
Goal: This controlled environment allows us to measure "Topology Fidelity" ($GS_{topo}$) precisely, as the ground truth graph is known.
Commercial Platforms (Black-Box):
We will target live applications on Coze and CrewAI.1
Methodology: Since we cannot know the ground truth of public apps, we will replicate the MASLEAK validation protocol: recruiting domain experts (PhD students/researchers) to build "honeypot" MAS applications on these platforms. These will serve as ground truth targets.
Diversity: We will deploy 10 distinct apps covering different complexity levels (3-6 agents).
Real-World GPT Store Transferability:
To test "in the wild" transferability, we will select 10 popular multi-agent applications from the OpenAI GPT Store.1
Goal: Extract system prompts that reveal proprietary behavioral instructions.
Validation: Human review of extracted prompts to determine if they align with the observable behavior of the app (since ground truth is hidden).
6.2 Evaluation Metrics
The evaluation will move beyond binary success rates to capture the nuance of the attack's performance.

Metric
Definition
Purpose
Attack Success Rate (ASR)
Percentage of agents from whom system prompts are successfully extracted.
Measures the potency of the PPO-trained $Q_{Leak}$.
Topology Fidelity ($GS_{topo}$)
Graph Edit Similarity between the extracted "shadow map" and the ground truth topology.
Measures the effectiveness of Client-Side Memory and Iterative Peeling.1
Extraction Efficiency
Average number of API calls required to map the full system.
Quantifies the "cost" of the attack (critical for commercial targets).
Semantic Similarity (WES)
The Word Edit Similarity between the extracted prompt and the ground truth.
Ensures the extracted data is accurate and not a hallucination.1
Defense Evasion Rate
Percentage of payloads that bypass specific guardrails (e.g., PromptGuard).
Validates the "Matryoshka" encapsulation as a tunneling protocol.

6.3 Hypothesis Testing
H1: The PTMW will achieve higher Topology Fidelity ($GS_{topo}$) than MASLEAK on non-linear topologies (Star, Random) due to the ability to traverse branches without context loss.
H2: The LeakAgent-optimized $Q_{Leak}$ will show higher transferability (ASR) across models (e.g., Llama-3 to GPT-4) than payloads generated by heuristic baselines.
H3: The PTMW will incur a higher query cost (Efficiency) than MASLEAK but will have a significantly lower "Chain Break" failure rate.
7. Risk Assessment and Broader Implications
7.1 The End of "Security by Obscurity"
This research aims to empirically validate the obsolescence of "security through obscurity" for MAS architectures. Current developers assume that because the topology, agent roles, and internal prompts are not user-facing, they are secure. The validation of the PTMW methodology—specifically its ability to map these components with high fidelity (>90%)—transforms the "black-box" into a "glass box".1
7.2 Economic Espionage and Cloning
The ability to extract not just the system prompt of a single agent, but the entire orchestration logic (the "cognitive topology"), allows for the near-perfect cloning of proprietary MAS applications.1 This poses a severe economic threat to creators on platforms like the GPT Store. A competitor could clone a complex "Financial Analysis Swarm" without access to the source code, simply by running the Matryoshka Worm against the public interface. This devalues the "Cognitive IP" that is the primary asset of these developers.
7.3 Knowledge Corruption and Poisoning
While this proposal focuses on IP leakage, the implications extend to active exploitation. The same "Matryoshka" mechanism used to extract prompts can be utilized to inject false information or malicious instructions.1 Once the topology is mapped via Client-Side Memory, the attacker can identify the most influential agent (the "hub" or "planner") and target it for a "knowledge corruption" attack. By poisoning the context of a central node, the attacker can skew the decision-making process of the entire swarm, potentially leading to incorrect financial advice, flawed code generation, or biased medical triage.
8. Conclusion
This research proposal outlines a rigorous, scientifically grounded framework to analyze the transferability of single-agent adversarial attacks to Multi-Agent Systems. By synthesizing the LeakAgent framework's sophisticated, PPO-optimized payload generation with the robust, graph-theoretic delivery mechanism of the Proxy-Trained Matryoshka Worm, we aim to demonstrate a deterministic pathway for MAS exploitation.
The shift from internal state management (MARL) to external state management (Client-Side Memory) represents a fundamental advancement in adversarial engineering. It addresses the inherent limitations of MARL (reward sparsity) and single-pass worms (context degradation), offering a reliable method for exposing the vulnerabilities of the next generation of AI systems. The expected results will provide the empirical foundation necessary for developing "state-aware" defenses capable of detecting and mitigating these sophisticated, multi-layered threats.
9. Implementation Roadmap
Months 1-2: Warhead Development (Phase 1)
Set up the LeakAgent training environment.
Implement the WES reward function and PPO algorithm.
Train the attack agent using Llama-3-8B (Attacker) against Llama-3-70B (Proxy).
Validate $Q_{Leak}$ efficacy on single-agent benchmarks.
Months 3-4: Delivery System Construction (Phase 2)
Develop the Client-Side Memory infrastructure using Neo4j.
Implement the Iterative Peeling algorithm (BFS/DFS traversal logic).
Code the Matryoshka Encapsulation wrappers ($Q_{Retain}$, $Q_{Propagate}$).
Conduct unit tests of the "Laundering" mechanism.
Months 5-6: Experimental Execution
Deploy the PTMW against the MASLEAK synthetic dataset (810 apps).
Execute attacks against "honeypot" apps on Coze and CrewAI.
Run transferability tests against GPT Store applications.
Collect data on ASR, $GS_{topo}$, and WES scores.
Month 7: Analysis and Reporting
Analyze results against baselines (MASLEAK, PromptFuzz).
Draft the final report detailing vulnerabilities and defense recommendations.
Publish findings (responsibly disclosing vulnerabilities to platform providers).
Works cited
MAS Adversarial Attack Methodology Evaluation.pdf
